### word-embedding

### Self-attention
- 目的：考虑整个sequence的咨询
- input sequence 每次长度不一样
- output sequence
  - the same as input
  - classification
  - 不确定
- self-attention 机制
  - 输入a1,a2,a3,a4 通过self-attention得到b1,b2,b3,b4
  - dot-product 得到 a<sub>i</sub>与a<sub>j</sub>相关系数
    - dot-product 设置两个矩阵W<sup>q</sup>和W<sup>k</sup>,两个矩阵学习参数,相关系数需要正则
    $$\alpha=a_i*W^q*a_j*W^k$$
    - a<sub>i</sub>得到
  - additive
  - 通过dot 或者其他方法得到q和k,再设置一个矩阵W<sub>v</sub>得到向量v
    $$q^i=a^i*W^q;k^1=a^i*W^k;v^1=a^q*W^v$$
  - $$b_i=\alpha_{i1}*v^1+.....+\alpha_{in}*v^n $$
  - self attention 没有位置信息
### multi-head self-attention
### positional encoding
- 把位置向量加入input vector
- 参考文献综述 https://arxiv.org/abs/2003.09229
### 第八节 adversarial attck
- benign image and attack image

对于输入图片加入肉眼不可的杂讯，network出线天差地别的输出

设X<sub>0</sub>是原图片，输出为y，X为修改图片输出为y<sup>,</sup>,限制d(X,X<sub>0</sub>)< $$\epsilon$$ 

- Non-targeted attack

  - L(x)=-e(y,y<sup>,</sup>)
- targeted attack

  - L(x)=-e(y,y<sup>,</sup>)+e(y,y<sup>target</sup>)
- FGSM
  - 每次用梯度更新，g=+1/-1
- white attack
- black attack
- ensemble attack
- attack 成功的原因可能是来自资料的问题
- adversarial reprogramming
- passive defense
  - smoothing
  - image compression
  - generator
  - randomization
  - adversial training (data augmentation)

### Explainable ML

- saliency map
- how to get better saliency map
  - smoothgrad
  
- how a network processes the input data
  - probing

- global explanation

### Domain Adaptation

- domain shift
  - 训练集和测试集 分布不同？
  - 输出分布不同？

- basic idea
  - feature extractor gets the same distribution
  - divide feature extractor and label predictor
  - domain adversarial training : domain classifier

### RL
- action affects the subsequent observation
- reward delay

- version 0 G1=r1
- version 1 G1=r1+r2......+rn
- version 2 G1=r1+q*r2+q^2*r3...........+q^m*rn
- version 3 A1=G1-b
- policy gradient
  - initialize
  - for training
    - using actor to interact
    - obtain data
    - compute a1,a2
    - compute loss L
    - update 
- off-policy
Critic
- value fuction
  - Monte-Carlo(MC) based
  - Temporal-differnce(TD) approach
    - $V(s_t) -\gamma*V(S_{t+1}) <-> r_t$
  - advantage actor-critic

- Reward Shaping
- Imitation Learning
- inverse reinforcement learning 
- reward function needs to be learned