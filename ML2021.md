### word-embedding

### Self-attention
- 目的：考虑整个sequence的咨询
- input sequence 每次长度不一样
- output sequence
  - the same as input
  - classification
  - 不确定
- self-attention 机制
  - 输入a1,a2,a3,a4 通过self-attention得到b1,b2,b3,b4
  - 
### 第八节 adversarial attck
benign image and attack image
对于输入图片加入肉眼不可的杂讯，network出线天差地别的输出

设X<sub>0</sub>是原图片，输出为y，X为修改图片输出为y<sup>,</sup>,限制d(X,X<sub>0</sub>)< $$\epsilon$$ 

- Non-targeted attack

  - L(x)=-e(y,y<sup>,</sup>)
- targeted attack

  - L(x)=-e(y,y<sup>,</sup>)+e(y,y<sup>target</sup>)
- FGSM
  - 每次用梯度更新，g=+1/-1
- white attack
- black attack
- ensemble attack
- attack 成功的原因可能是来自资料的问题
- adversarial reprogramming
- passive defense
  - smoothing
  - image compression
  - generator
  - randomization
  - adversial training (data augmentation)

### Explainable ML

- saliency map
- how to get better saliency map
  - smoothgrad
  
- how a network processes the input data
  - probing

- global explanation

### Domain Adaptation

- domain shift
  - 训练集和测试集 分布不同？
  - 输出分布不同？

- basic idea
  - feature extractor gets the same distribution
  - divide feature extractor and label predictor
  - domain adversarial training : domain classifier

### RL
- action affects the subsequent observation
- reward delay

- version 0 G1=r1
- version 1 G1=r1+r2......+rn
- version 2 G1=r1+q*r2+q^2*r3...........+q^m*rn
- version 3 A1=G1-b
- policy gradient
  - initialize
  - for training
    - using actor to interact
    - obtain data
    - compute a1,a2
    - compute loss L
    - update 
- off-policy
Critic
- value fuction
  - Monte-Carlo(MC) based
  - Temporal-differnce(TD) approach
    - $V(s_t) -\gamma*V(S_{t+1}) <-> r_t$
  - advantage actor-critic

- Reward Shaping
- Imitation Learning
- inverse reinforcement learning 
- reward function needs to be learned