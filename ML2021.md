### 第八节 adversarial attck
benign image and attack image
对于输入图片加入肉眼不可的杂讯，network出线天差地别的输出

设X<sub>0</sub>是原图片，输出为y，X为修改图片输出为y<sup>,</sup>,限制d(X,X<sub>0</sub>)< $$\epsilon$$ 

- Non-targeted attack

  - L(x)=-e(y,y<sup>,</sup>)
- targeted attack

  - L(x)=-e(y,y<sup>,</sup>)+e(y,y<sup>target</sup>)
- FGSM
  - 每次用梯度更新，g=+1/-1
- white attack
- black attack
- ensemble attack
- attack 成功的原因可能是来自资料的问题
- adversarial reprogramming
- passive defense
  - smoothing
  - image compression
  - generator
  - randomization
  - adversial training (data augmentation)

### Explainable ML

- saliency map
- how to get better saliency map
  - smoothgrad
  
- how a network processes the input data
  - probing

- global explanation

### Domain Adaptation

- domain shift
  - 训练集和测试集 分布不同？
  - 输出分布不同？

- basic idea
  - feature extractor gets the same distribution
  - divide feature extractor and label predictor
  - domain adversarial training : domain classifier